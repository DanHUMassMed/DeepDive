{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System level imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# ##### SET SYS PATH TO WHERE THE CODE IS. #####\n",
    "# my_local_development_dir/llm_analyst\n",
    "# Note: Putting our code first in the sys path will make sure it gets picked up\n",
    "llm_analyst_base_dir='/Users/dan/Code/LLM/DeepDive/backend'\n",
    "sys.path.insert(0, llm_analyst_base_dir)\n",
    "\n",
    "\n",
    "# Setting the USER_AGENT to fix warning with langchain_community code\n",
    "# WARNING:langchain_community.utils.user_agent:USER_AGENT\n",
    "user_agent = (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "              \"(KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0\")\n",
    "os.environ['USER_AGENT'] = user_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ../backend/app/search_methods/inte import \n",
    "query = \"Python programming !w\"\n",
    "search_response = tavily_search(query, max_results=5)\n",
    "#Display the search results\n",
    "for result in search_response:\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"URL: {result['href']}\")\n",
    "    print(f\"Body: {result['body']}\\n\")\n",
    "    print(f\"Body len: {len(result['body'])}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/7cv336vx66v_c3vj7vwrpt6c0000gn/T/ipykernel_89499/2327478108.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/var/folders/jq/7cv336vx66v_c3vj7vwrpt6c0000gn/T/ipykernel_89499/2327478108.py:12: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=llm, memory=memory)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize memory to store the conversation history\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Initialize the LLM with memory\n",
    "llm = ChatOllama(model = \"deepseek-r1:32b\",)\n",
    "\n",
    "# Create the conversation chain that uses memory\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I identify the numbers involved in the problem: 2 and 2.\n",
      "\n",
      "Next, I perform the addition operation by combining these two numbers together.\n",
      "\n",
      "Finally, adding them results in a total of 4.\n",
      "</think>\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "We need to find the sum of two numbers:\n",
      "\n",
      "\\[\n",
      "2 + 2\n",
      "\\]\n",
      "\n",
      "Adding them together gives:\n",
      "\n",
      "\\[\n",
      "2 + 2 = \\boxed{4}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"human\", \"What is two plus two\"),\n",
    "]\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "I need to add three to the last number provided, which is 2. Adding 3 to 2 gives me 5.\n",
      "</think>\n",
      "\n",
      "Sure! Let's add three to the last number in your response.\n",
      "\n",
      "**Given:**\n",
      "Last response number = 2\n",
      "\n",
      "**Calculation:**\n",
      "\\[\n",
      "2 + 3 = \\boxed{5}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"human\", \"Add three to the last response\"),\n",
    "]\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "I need to add three to the last response provided. The previous answer was 5, so I will calculate 5 plus 3.\n",
      "\n",
      "Adding these together gives me a total of 8.\n",
      "</think>\n",
      "\n",
      "Certainly! Let's go through the addition step by step.\n",
      "\n",
      "**Given:**\n",
      "- Previous response: \\( 5 \\)\n",
      "- To add: \\( 3 \\)\n",
      "\n",
      "**Calculation:**\n",
      "\\[\n",
      "5 + 3 = 8\n",
      "\\]\n",
      "\n",
      "**Final Answer:**\n",
      "\\[\n",
      "\\boxed{8}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"human\", \"Add three to the last response\"),\n",
    "]\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-dive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
