{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"deepseek-r1:32b\", model_provider=\"ollama\")\n",
    "model = init_chat_model(\"llama3.3\", model_provider=\"ollama\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = init_chat_model(\"deepseek-r1:32b\", model_provider=\"ollama\")\n",
    "model = init_chat_model(\"llama3.3\", model_provider=\"ollama\")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Async function for node:\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define graph as before:\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "aapp = workflow.compile(checkpointer=MemorySaver())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "# Async invocation:\n",
    "output = await aapp.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"What's my name?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "# Async invocation:\n",
    "output = await aapp.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a System Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "# Async invocation:\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"deepseek-r1:32b\", model_provider=\"ollama\")\n",
    "model = init_chat_model(\"llama3.3\", model_provider=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=6_500_000,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke({\"messages\": trimmed_messages})\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "import sqlite3\n",
    "import aiosqlite\n",
    "\n",
    "\n",
    "db_path = 'checkpoints.db'\n",
    "conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "\n",
    "#memory = SqliteSaver(conn)\n",
    "# Does not work\n",
    "#memory = AsyncSqliteSaver(conn)\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "        \n",
    "print(type(app))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc12\"}}\n",
    "inputs = {\"messages\": [HumanMessage(content=\"What is 1+1?\")]}\n",
    "\n",
    "async with AsyncSqliteSaver.from_conn_string(db_path) as saver:\n",
    "    graph = workflow.compile(checkpointer=saver)\n",
    "    async for event in graph.astream_events(inputs, config, version=\"v1\"):\n",
    "        pprint(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS COULD BE THE ONE I NEED\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc12\"}}\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Hi! I'm Bob.\")]}\n",
    "\n",
    "async with AsyncSqliteSaver.from_conn_string(db_path) as saver:\n",
    "    graph = workflow.compile(checkpointer=saver)        \n",
    "    async for chunk, metadata in graph.astream(\n",
    "        inputs,\n",
    "        config,\n",
    "        stream_mode=\"messages\",\n",
    "    ):\n",
    "        if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "            print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\":  \"abc12\"}}\n",
    "\n",
    "\n",
    "\n",
    "with SqliteSaver.from_conn_string(db_path) as checkpointer:\n",
    "    graph = workflow.compile(checkpointer=checkpointer)\n",
    "    state_history = graph.get_state_history(config) \n",
    "    values = next(state_history).values  \n",
    "    print(values) \n",
    "    print(len(values['messages']))\n",
    "    for message in values['messages']:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"Human: {message.content}\")\n",
    "        else:\n",
    "            print(f\"AI:    {message.content}\")\n",
    "    \n",
    "    #state_history_list = list(state_history)\n",
    "    #print(state_history_list)\n",
    "    # for state in state_history:\n",
    "    #     print(state)\n",
    "    #     # print(f\"Step: {state.step}\")\n",
    "    #     # print(f\"Created At: {state.created_at}\")\n",
    "    #     # print(f\"Values: {state.values}\")\n",
    "    #     # print(\"-----\")\n",
    " \n",
    "    #list(graph.get_state_history(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is my name\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "async for chunk, metadata in app.astream(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "builder = StateGraph(int)\n",
    "builder.add_node(\"add_one\", lambda x: x + 1)\n",
    "builder.set_entry_point(\"add_one\")\n",
    "builder.set_finish_point(\"add_one\")\n",
    "async with AsyncSqliteSaver.from_conn_string(\"checkpoints.db\") as memory:\n",
    "    graph = builder.compile(checkpointer=memory)\n",
    "    coro = graph.ainvoke(1, {\"configurable\": {\"thread_id\": \"thread-1\"}})\n",
    "    print(asyncio.run(coro))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import AIMessage, HumanMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = \"Answer all questions to the best of your ability. Answer concisely but correctly. If you do not know the answer, just say 'I don’t know.'\"\n",
    "model = init_chat_model(\"deepseek-r1:32b\", model_provider=\"ollama\")\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=6_500_000,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "graph = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke({\"messages\": trimmed_messages})\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "graph.add_edge(START, \"model\")\n",
    "graph.add_node(\"model\", call_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IN get_chat_interactions_count\")\n",
    "interactions_count = 0\n",
    "db_path=\"/Users/dan/Code/LLM/DeepDive/backend/resources/checkpoints.db\"\n",
    "chat_id=\"ea9123ff-9a8e-46c0-a53f-22b8f88e3202\"\n",
    "\n",
    "with SqliteSaver.from_conn_string(db_path) as checkpointer:            \n",
    "    config = {\"configurable\": {\"thread_id\": chat_id}}\n",
    "    print(\"before compiled_graph\")\n",
    "    compiled_graph = graph.compile(checkpointer=checkpointer)\n",
    "    print(\"after compiled_graph\")\n",
    "    state_history = compiled_graph.get_state_history(config) \n",
    "    print(\"after state_history\")\n",
    "    last_interaction = next(state_history, None)\n",
    "    print(\"after last_interaction\")\n",
    "    if last_interaction:\n",
    "        values = last_interaction.values  \n",
    "        if 'messages' in values:\n",
    "            interactions_count = len(values['messages'])\n",
    "            \n",
    "interactions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/Users/dan/Code/LLM/DeepDive/backend')\n",
    "\n",
    "import app.app_session\n",
    "file_path = app.app_session.__file__\n",
    "parent_directory = os.path.dirname(file_path)\n",
    "grandparent_directory = os.path.dirname(parent_directory)\n",
    "grandparent_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET CONVERSATION HISTORY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import AIMessage, HumanMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "model = init_chat_model(\"llama3.2:1b\", model_provider=\"ollama\")\n",
    "system_prompt = \"Do your best to answer correctly\"\n",
    "\n",
    "def create_graph():\n",
    "    #TODO THIS DOES NOT LOOK RIGHT    \n",
    "    trimmer = trim_messages(\n",
    "        max_tokens=6_500_000,\n",
    "        strategy=\"last\",\n",
    "        token_counter=model,\n",
    "        include_system=True,\n",
    "        allow_partial=False,\n",
    "        start_on=\"human\",\n",
    "    )\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    graph = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "    def call_model(state: MessagesState):\n",
    "        trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "        prompt = prompt_template.invoke({\"messages\": trimmed_messages})\n",
    "        response = model.invoke(prompt)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    graph.add_edge(START, \"model\")\n",
    "    graph.add_node(\"model\", call_model)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "graph = create_graph()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_path = \"/Users/dan/Code/LLM/DeepDive/backend/resources/checkpoints.db\"\n",
    "\n",
    "def get_chat_interactions_count(chat_id):\n",
    "    interactions_count = 0\n",
    "    with SqliteSaver.from_conn_string(db_path) as checkpointer:            \n",
    "        config = {\"configurable\": {\"thread_id\": chat_id}}\n",
    "        compiled_graph = graph.compile(checkpointer=checkpointer)\n",
    "        state_history = compiled_graph.get_state_history(config) \n",
    "        last_interaction = next(state_history, None)\n",
    "        if last_interaction:\n",
    "            values = last_interaction.values  \n",
    "            if 'messages' in values:\n",
    "                interactions_count = len(values['messages'])\n",
    "    return interactions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_interactions(chat_id):\n",
    "    interactions=[]\n",
    "    with SqliteSaver.from_conn_string(db_path) as checkpointer:            \n",
    "        config = {\"configurable\": {\"thread_id\": chat_id}}\n",
    "        compiled_graph = graph.compile(checkpointer=checkpointer)\n",
    "        state_history = compiled_graph.get_state_history(config) \n",
    "        last_interaction = next(state_history, None)\n",
    "        if last_interaction:\n",
    "            values = last_interaction.values  \n",
    "            if 'messages' in values:\n",
    "                for message in values['messages']:\n",
    "                    #print(type(message))\n",
    "                    #print(message.content)\n",
    "                    interaction_type = 'user' if isinstance(message,HumanMessage) else 'ai'\n",
    "                    interaction = {'type':interaction_type, 'content':message.content}\n",
    "                    interactions.append(interaction)\n",
    "    return interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'user', 'content': 'hello'},\n",
       " {'type': 'ai',\n",
       "  'content': '<think>\\nOkay, so I\\'m trying to figure out what \"hello\" means in this context. The user greeted me with \"hello,\" and the assistant responded by asking how they can assist today. It seems like a standard interaction where someone starts a conversation.\\n\\nI wonder if there\\'s more to it. Maybe the user is testing the AI or just initiating a chat. Since the response was pretty straightforward, I don\\'t think there\\'s any hidden meaning here. The assistant is prompting for further assistance, so my next step should be to provide whatever help they need based on their follow-up question.\\n\\nI guess I should make sure that in my responses, I\\'m clear and helpful without overcomplicating things. Since the user said \"hello,\" it might be a way to get the conversation started, so I need to be ready for any questions or topics they bring up next.\\n</think>\\n\\nThe user greeted with \"hello,\" which is a standard way to start a conversation. The assistant responded by inviting further interaction, indicating readiness to assist. Therefore, the appropriate response would be:\\n\\nHow can I assist you today?'},\n",
       " {'type': 'user', 'content': 'my name is dan'},\n",
       " {'type': 'ai',\n",
       "  'content': '<think>\\nAlright, let\\'s see. The user just told me their name is Dan. They said, \"my name is dan.\" So, my first thought is that they\\'re introducing themselves. It might be their way of starting a conversation or providing information for further interaction.\\n\\nI should respond in a friendly manner to make them feel welcome. Maybe I can acknowledge their name and offer assistance. Since they didn\\'t ask anything specific yet, keeping the response open-ended would encourage them to continue the conversation.\\n\\nPerhaps something like, \"Hello Dan! How can I assist you today?\" That way, I\\'m acknowledging their introduction and inviting them to share what they need help with.\\n</think>\\n\\nHello Dan! How can I assist you today?'},\n",
       " {'type': 'user', 'content': 'what is my name'},\n",
       " {'type': 'ai',\n",
       "  'content': '<think>\\nAlright, the user has just asked, \"what is my name.\" Considering our previous conversation, they had introduced themselves as Dan. So now they\\'re asking for their own name.\\n\\nI should think about how to respond appropriately. They might be testing if I remember their name or perhaps trying to understand how AI handles such information.\\n\\nIn this case, since they provided their name earlier, it makes sense to confirm that their name is Dan and offer further assistance. It\\'s important to acknowledge their input positively and maintain a helpful tone.\\n</think>\\n\\nHello Dan! Your name is Dan. How can I assist you today?'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_id=\"ea9123ff-9a8e-46c0-a53f-22b8f88e3202\"\n",
    "get_chat_interactions(chat_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-dive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
