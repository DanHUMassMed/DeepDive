{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"deepseek-r1:32b\", model_provider=\"ollama\")\n",
    "model = init_chat_model(\"llama3.3\", model_provider=\"ollama\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = init_chat_model(\"deepseek-r1:32b\", model_provider=\"ollama\")\n",
    "model = init_chat_model(\"llama3.3\", model_provider=\"ollama\")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Async function for node:\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define graph as before:\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "aapp = workflow.compile(checkpointer=MemorySaver())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "# Async invocation:\n",
    "output = await aapp.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"What's my name?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "# Async invocation:\n",
    "output = await aapp.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a System Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "# Async invocation:\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"deepseek-r1:32b\", model_provider=\"ollama\")\n",
    "model = init_chat_model(\"llama3.3\", model_provider=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=6_500_000,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke({\"messages\": trimmed_messages})\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "import sqlite3\n",
    "import aiosqlite\n",
    "\n",
    "\n",
    "db_path = 'checkpoints.db'\n",
    "conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "\n",
    "#memory = SqliteSaver(conn)\n",
    "# Does not work\n",
    "#memory = AsyncSqliteSaver(conn)\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "        \n",
    "print(type(app))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc12\"}}\n",
    "inputs = {\"messages\": [HumanMessage(content=\"What is 1+1?\")]}\n",
    "\n",
    "async with AsyncSqliteSaver.from_conn_string(db_path) as saver:\n",
    "    graph = workflow.compile(checkpointer=saver)\n",
    "    async for event in graph.astream_events(inputs, config, version=\"v1\"):\n",
    "        pprint(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS COULD BE THE ONE I NEED\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc12\"}}\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Hi! I'm Bob.\")]}\n",
    "\n",
    "async with AsyncSqliteSaver.from_conn_string(db_path) as saver:\n",
    "    graph = workflow.compile(checkpointer=saver)        \n",
    "    async for chunk, metadata in graph.astream(\n",
    "        inputs,\n",
    "        config,\n",
    "        stream_mode=\"messages\",\n",
    "    ):\n",
    "        if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "            print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\":  \"abc12\"}}\n",
    "\n",
    "\n",
    "\n",
    "with SqliteSaver.from_conn_string(db_path) as checkpointer:\n",
    "    graph = workflow.compile(checkpointer=checkpointer)\n",
    "    state_history = graph.get_state_history(config) \n",
    "    values = next(state_history).values  \n",
    "    print(values) \n",
    "    print(len(values['messages']))\n",
    "    for message in values['messages']:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"Human: {message.content}\")\n",
    "        else:\n",
    "            print(f\"AI:    {message.content}\")\n",
    "    \n",
    "    #state_history_list = list(state_history)\n",
    "    #print(state_history_list)\n",
    "    # for state in state_history:\n",
    "    #     print(state)\n",
    "    #     # print(f\"Step: {state.step}\")\n",
    "    #     # print(f\"Created At: {state.created_at}\")\n",
    "    #     # print(f\"Values: {state.values}\")\n",
    "    #     # print(\"-----\")\n",
    " \n",
    "    #list(graph.get_state_history(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is my name\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "async for chunk, metadata in app.astream(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "builder = StateGraph(int)\n",
    "builder.add_node(\"add_one\", lambda x: x + 1)\n",
    "builder.set_entry_point(\"add_one\")\n",
    "builder.set_finish_point(\"add_one\")\n",
    "async with AsyncSqliteSaver.from_conn_string(\"checkpoints.db\") as memory:\n",
    "    graph = builder.compile(checkpointer=memory)\n",
    "    coro = graph.ainvoke(1, {\"configurable\": {\"thread_id\": \"thread-1\"}})\n",
    "    print(asyncio.run(coro))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import AIMessage, HumanMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = \"Answer all questions to the best of your ability. Answer concisely but correctly. If you do not know the answer, just say 'I don’t know.'\"\n",
    "model = init_chat_model(\"deepseek-r1:32b\", model_provider=\"ollama\")\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=6_500_000,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "graph = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke({\"messages\": trimmed_messages})\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "graph.add_edge(START, \"model\")\n",
    "graph.add_node(\"model\", call_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IN get_chat_interactions_count\")\n",
    "interactions_count = 0\n",
    "db_path=\"/Users/dan/Code/LLM/DeepDive/backend/resources/checkpoints.db\"\n",
    "chat_id=\"ea9123ff-9a8e-46c0-a53f-22b8f88e3202\"\n",
    "\n",
    "with SqliteSaver.from_conn_string(db_path) as checkpointer:            \n",
    "    config = {\"configurable\": {\"thread_id\": chat_id}}\n",
    "    print(\"before compiled_graph\")\n",
    "    compiled_graph = graph.compile(checkpointer=checkpointer)\n",
    "    print(\"after compiled_graph\")\n",
    "    state_history = compiled_graph.get_state_history(config) \n",
    "    print(\"after state_history\")\n",
    "    last_interaction = next(state_history, None)\n",
    "    print(\"after last_interaction\")\n",
    "    if last_interaction:\n",
    "        values = last_interaction.values  \n",
    "        if 'messages' in values:\n",
    "            interactions_count = len(values['messages'])\n",
    "            \n",
    "interactions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/Users/dan/Code/LLM/DeepDive/backend')\n",
    "\n",
    "import app.app_session\n",
    "file_path = app.app_session.__file__\n",
    "parent_directory = os.path.dirname(file_path)\n",
    "grandparent_directory = os.path.dirname(parent_directory)\n",
    "grandparent_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET CONVERSATION HISTORY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import AIMessage, HumanMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "model = init_chat_model(\"llama3.2:1b\", model_provider=\"ollama\")\n",
    "system_prompt = \"Do your best to answer correctly\"\n",
    "\n",
    "def create_graph():\n",
    "    #TODO THIS DOES NOT LOOK RIGHT    \n",
    "    trimmer = trim_messages(\n",
    "        max_tokens=6_500_000,\n",
    "        strategy=\"last\",\n",
    "        token_counter=model,\n",
    "        include_system=True,\n",
    "        allow_partial=False,\n",
    "        start_on=\"human\",\n",
    "    )\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    graph = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "    def call_model(state: MessagesState):\n",
    "        trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "        prompt = prompt_template.invoke({\"messages\": trimmed_messages})\n",
    "        response = model.invoke(prompt)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    graph.add_edge(START, \"model\")\n",
    "    graph.add_node(\"model\", call_model)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "graph = create_graph()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_path = \"/Users/dan/Code/LLM/DeepDive/backend/resources/checkpoints.db\"\n",
    "\n",
    "def get_chat_interactions_count(chat_id):\n",
    "    interactions_count = 0\n",
    "    with SqliteSaver.from_conn_string(db_path) as checkpointer:            \n",
    "        config = {\"configurable\": {\"thread_id\": chat_id}}\n",
    "        compiled_graph = graph.compile(checkpointer=checkpointer)\n",
    "        state_history = compiled_graph.get_state_history(config) \n",
    "        last_interaction = next(state_history, None)\n",
    "        if last_interaction:\n",
    "            values = last_interaction.values  \n",
    "            if 'messages' in values:\n",
    "                interactions_count = len(values['messages'])\n",
    "    return interactions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_interactions(chat_id):\n",
    "    interactions=[]\n",
    "    with SqliteSaver.from_conn_string(db_path) as checkpointer:            \n",
    "        config = {\"configurable\": {\"thread_id\": chat_id}}\n",
    "        compiled_graph = graph.compile(checkpointer=checkpointer)\n",
    "        state_history = compiled_graph.get_state_history(config) \n",
    "        last_interaction = next(state_history, None)\n",
    "        if last_interaction:\n",
    "            values = last_interaction.values  \n",
    "            if 'messages' in values:\n",
    "                for message in values['messages']:\n",
    "                    #print(type(message))\n",
    "                    #print(message.content)\n",
    "                    interaction_type = 'user' if isinstance(message,HumanMessage) else 'ai'\n",
    "                    interaction = {'type':interaction_type, 'content':message.content}\n",
    "                    interactions.append(interaction)\n",
    "    return interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_id=\"ea9123ff-9a8e-46c0-a53f-22b8f88e3202\"\n",
    "get_chat_interactions(chat_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import time\n",
    "\n",
    "def searxng_search(query, max_results=5, retries=3, timeout=10):\n",
    "    searxng_url = \"https://searxng.danhiggins.org/search\"\n",
    "    \n",
    "    # Define your search query parameters\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    # Initialize variables for retry logic\n",
    "    attempt = 0\n",
    "    last_exception = None\n",
    "    \n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            # Send the request to the SearxNG instance with timeout\n",
    "            response = requests.get(searxng_url, params=params, timeout=timeout)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Parse the JSON response\n",
    "                json_response = response.json()\n",
    "                \n",
    "                # Get results with a fallback to an empty list\n",
    "                json_response_results = json_response.get(\"results\", [])\n",
    "                \n",
    "                # Limit results if needed\n",
    "                json_response_results = json_response_results[:max_results]\n",
    "                \n",
    "                # Map the response\n",
    "                json_response_mapped = [\n",
    "                    {\n",
    "                        'href': item.get('url', ''),\n",
    "                        'title': item.get('title', ''),\n",
    "                        'body': item.get('content', '')\n",
    "                    }\n",
    "                    for item in json_response_results\n",
    "                    if any(key in item for key in ('url', 'title', 'content'))\n",
    "                ]\n",
    "                        \n",
    "                return json_response_mapped\n",
    "            else:\n",
    "                last_exception = f\"Error: HTTP {response.status_code}\"\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            last_exception = f\"Timeout after {timeout} seconds\"\n",
    "        except RequestException as e:\n",
    "            last_exception = f\"Request failed: {str(e)}\"\n",
    "        \n",
    "        # Increment attempt count and wait before retrying (with exponential backoff)\n",
    "        attempt += 1\n",
    "        if attempt < retries:\n",
    "            wait_time = min(2 ** attempt, 30)  # Max wait time of 30 seconds\n",
    "            print(f\"Attempt {attempt} failed: {last_exception}. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    # If all retries failed, raise an exception with the last error\n",
    "    raise Exception(f\"All {retries} attempts failed. Last error: {last_exception}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'href': 'https://en.wikipedia.org/wiki/Bernie_Sanders',\n",
       "  'title': 'Bernie Sanders - Wikipedia',\n",
       "  'body': '3 days ago - Bernard Sanders was born on September 8, 1941, in the Brooklyn borough of New York City. His father, Elias Ben Yehuda Sanders (1904–1962), a Polish-Jewish immigrant, was born in Słopnice, a town in Austrian Galicia that was then part of the Austro-Hungarian Empire and is now in Poland.'},\n",
       " {'href': 'https://www.sanders.senate.gov/about-bernie/',\n",
       "  'title': 'About Bernie - Senator Bernie Sanders',\n",
       "  'body': 'Bernie Sanders is serving his fourth term in the U.S. Senate after winning re-election in 2024. His previous 16 years in the House of Representatives make him the longest serving independent member of Congress in American history. Born in 1941 in Brooklyn, Sanders attended James Madison High ...'},\n",
       " {'href': 'https://www.britannica.com/biography/Bernie-Sanders',\n",
       "  'title': 'Bernie Sanders | Biography & Facts | Britannica',\n",
       "  'body': '2 weeks ago - Bernie Sanders, American politician who represented Vermont in the U.S. Senate from 2007. Previously he served as the mayor of Burlington (1981–89) and as a member of the U.S. House of Representatives (1991–2007). He sought the Democratic presidential nomination in the U.S. presidential ...'},\n",
       " {'href': 'https://berniesanders.com/about/',\n",
       "  'title': 'Meet Bernie | Bernie Sanders Official Website',\n",
       "  'body': \"Bernie Sanders is a U.S. Senator from Vermont and candidate to become the next President of the United States. In 2006, he was elected to the U.S. Senate after 16 years as Vermont's sole congressman in the House of Representatives. Bernie is now serving his third term in the U.S. Senate after winning re-election in 2018 with 67 percent of the ...\"},\n",
       " {'href': 'https://www.pbs.org/newshour/politics/drawing-huge-crowds-bernie-sanders-emerges-as-the-leader-of-the-anti-trump-resistance',\n",
       "  'title': 'Drawing huge crowds, Bernie Sanders emerges as the leader of the anti-Trump resistance | PBS News',\n",
       "  'body': '4 days ago - Ocasio-Cortez, a longtime Sanders ally, said she would join him on the road in the coming weeks. She’s also planning solo appearances in Republican-held congressional districts in Pennsylvania and New York — and perhaps others in places where Republicans have declined to hold in-person town halls where they might face protests. “It’s not about whether Bernie should or shouldn’t be doing this. It’s about that we all should,” she said. “But he is ...'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query =\"who is bernie sanders\"\n",
    "json_response = searxng_search(query)\n",
    "print(len(json_response))\n",
    "json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from duckduckgo_search import DDGS\n",
    "from duckduckgo_search.exceptions import DuckDuckGoSearchException, RatelimitException, TimeoutException\n",
    "\n",
    "\n",
    "def ddg_search(query, max_results=5, retries=3, timeout=10):\n",
    "    \"\"\"Perform a search using DuckDuckGo's API with retry and timeout handling.\"\"\"\n",
    "    search_response = []\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            ddg = DDGS(timeout=timeout)\n",
    "            search_response = ddg.text(\n",
    "                query,\n",
    "                region=\"wt-wt\",\n",
    "                safesearch=\"off\",\n",
    "                timelimit=\"y\",\n",
    "                backend=\"lite\",\n",
    "                max_results=max_results,\n",
    "            )\n",
    "            break  # Exit loop if search is successful\n",
    "        except RatelimitException as e:\n",
    "            attempt += 1\n",
    "            wait_time = min(2 ** attempt, 30)  # Exponential backoff with a max wait time of 30 seconds\n",
    "            #logger.debug(f\"Rate limit exceeded: {e}. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "        except TimeoutException as e:\n",
    "            attempt += 1\n",
    "            wait_time = min(2 ** attempt, 30)\n",
    "            #logger.debug(f\"Timeout error: {e}. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "        except DuckDuckGoSearchException as e:\n",
    "            attempt += 1\n",
    "            wait_time = min(2 ** attempt, 30)\n",
    "            #logger.debug(f\"DuckDuckGo search error: {e}. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "    else:\n",
    "        #logger.debug(\"Maximum retries reached. Returning empty search response.\")\n",
    "        pass\n",
    "    return search_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Drawing huge crowds, Bernie Sanders emerges as the leader of the anti ...',\n",
       "  'href': 'https://www.pbs.org/newshour/politics/drawing-huge-crowds-bernie-sanders-emerges-as-the-leader-of-the-anti-trump-resistance',\n",
       "  'body': \"WARREN, Mich. (AP) — Bernie Sanders is standing alone on the back of a pickup truck shouting into a bullhorn. He's facing several hundred ecstatic voters huddled outside a suburban Detroit ...\"},\n",
       " {'title': 'Bernie Sanders takes leadership of the anti-Trump resistance - AP News',\n",
       "  'href': 'https://apnews.com/article/bernie-sanders-democrats-trump-c213d5ae42737c956d46f6f7f17e5abd',\n",
       "  'body': \"WARREN, Mich. (AP) — Bernie Sanders is standing alone on the back of a pickup truck shouting into a bullhorn. He's facing several hundred ecstatic voters huddled outside a suburban Detroit high school — the group that did not fit inside the high school's gym or two overflow rooms. The crowd screams in delight when he tells them that a ...\"},\n",
       " {'title': 'Bernie Sanders steps into leadership of the anti-Trump resistance',\n",
       "  'href': 'https://abcnews.go.com/Politics/wireStory/drawing-huge-crowds-bernie-sanders-steps-leadership-anti-119623034',\n",
       "  'body': 'At 83 years old, Bernie Sanders has emerged as an unlikely leader of the Democratic resistance to Donald Trump WARREN, Mich. -- Bernie Sanders is standing alone on the back of a pickup truck ...'},\n",
       " {'title': 'Bernie Sanders - APNews.ca',\n",
       "  'href': 'https://apnews.ca/biography/bernie-sanders/',\n",
       "  'body': 'Learn about Bernie Sanders, a prominent American politician who has shaped local and national political landscapes as a mayor, congressman, senator, and presidential candidate. Discover his early life, education, activism, achievements, and challenges in this comprehensive biography.'},\n",
       " {'title': 'Drawing huge crowds, Bernie Sanders steps into leadership of ... - Yahoo',\n",
       "  'href': 'https://www.yahoo.com/news/drawing-huge-crowds-bernie-sanders-040333889.html',\n",
       "  'body': \"Bernie Sanders is standing alone on the back of a pickup truck shouting into a bullhorn. At 83 years old, Sanders is not running for president again. In tearing into Trump's seizure of power and ...\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query =\"who is bernie sanders\"\n",
    "ddg_search(query, max_results=5, retries=3, timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-dive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
